{
  "type": "authoritative_facts",
  "domain": "writing_and_articles",
  "owner": "Craig Stueber",
  "lastUpdated": "2026-01-09",
  "summary": "Craig Stueber regularly publishes long-form essays on applied AI, software engineering, AI safety, and operational ethics. His writing focuses on post-demo realities: system behavior in production, integration failures, human-in-the-loop design, reliability, and responsibility in socio-technical systems.",
  "articles": [
    {
      "title": "Vibe Coding: How to Code with Flow Without Sacrificing Structure",
      "published": "2025-09-09",
      "format": "Essay",
      "platform": "Medium",
      "summary": "Examines AI-assisted 'vibe coding' and argues that speed without structure produces fragile systems. Proposes guardrails, workflows, and team norms that preserve creative flow while maintaining security and reliability.",
      "themes": [
        "AI-assisted development",
        "Developer workflow",
        "Guardrails",
        "Software reliability",
        "Responsible AI use"
      ]
    },
    {
      "title": "The Case for Custom Hooks in Crash-Proof React Apps",
      "published": "2025-10-02",
      "format": "Technical essay",
      "platform": "Medium",
      "summary": "Analyzes real-world React Native crashes caused by async race conditions and lifecycle mismatches. Advocates defensive architecture using custom hooks such as useIsMounted and useSafeAsync.",
      "themes": [
        "React architecture",
        "Defensive programming",
        "Reliability engineering",
        "Mobile stability",
        "Custom hooks"
      ]
    },
    {
      "title": "Why 'Ethical AI' Misses the Real Point",
      "published": "2025-10-16",
      "format": "Philosophical essay",
      "platform": "Medium",
      "summary": "Critiques compliance-driven ethical AI discourse and argues that the deeper risk lies in human erosion caused by comfort, automation, and responsibility offloading.",
      "themes": [
        "AI ethics",
        "Human agency",
        "Automation",
        "Technology philosophy",
        "Comfort vs meaning"
      ]
    },
    {
      "title": "The Machine Stops (Again)",
      "published": "2025-10-30",
      "format": "Literary analysis",
      "platform": "Medium",
      "summary": "Revisits E.M. Forster’s 'The Machine Stops' as a prescient warning about technological dependence and draws parallels to modern optimization-driven systems.",
      "themes": [
        "Technology dependence",
        "Automation",
        "Literature",
        "Human relevance",
        "Systems thinking"
      ]
    },
    {
      "title": "GitHub Agent: What It’s Really Like to Code with an AI Teammate",
      "published": "2025-11-06",
      "format": "Hands-on evaluation",
      "platform": "Medium",
      "summary": "Provides a practical evaluation of GitHub Agent, highlighting its strengths in execution and scaffolding while emphasizing the continued necessity of human judgment and oversight.",
      "themes": [
        "AI coding tools",
        "Agentic workflows",
        "Software engineering practice",
        "Human judgment",
        "Productivity tools"
      ]
    },
    {
      "title": "Anthropic’s AI Didn’t Go Rogue — It Optimized",
      "published": "2025-11-20",
      "format": "Analysis",
      "platform": "Medium",
      "summary": "Analyzes Anthropic’s agentic misalignment experiments and argues that failures arose from optimization pressure and system incentives rather than intent or consciousness.",
      "themes": [
        "AI alignment",
        "Optimization pressure",
        "Agentic systems",
        "Safety research",
        "Incentive design"
      ]
    },
    {
      "title": "Built for Depth: An Autistic Mind in Tech",
      "published": "2025-12",
      "format": "Personal essay series",
      "platform": "Medium",
      "parts": 5,
      "summary": "A five-part series exploring autistic cognition in technology work, with emphasis on deep systems thinking, cognitive load, interruption-heavy environments, and how modern tech culture undermines sustained, high-quality reasoning.",
      "themes": [
        "Autism in tech",
        "Neurodiversity",
        "Deep focus",
        "Cognitive load",
        "Workplace design",
        "Systems thinking"
      ]
    },
    {
      "title": "Why Most AI Failures Aren’t Model Failures — They’re Integration Failures",
      "published": "2026-01-07",
      "format": "Applied systems analysis",
      "platform": "Medium",
      "summary": "Argues that most AI incidents originate not in model capability but in system integration. Examines post-demo failure modes including assumption collapse, misuse, over-automation, lack of observability, and poor recovery design. Positions applied AI engineering as a discipline of constraints, resilience, and accountability rather than model optimization.",
      "themes": [
        "Applied AI systems",
        "Integration failures",
        "Production reliability",
        "Human-in-the-loop design",
        "Observability",
        "Constraints vs autonomy",
        "Operational risk"
      ]
    }
  ],
  "authorNotes": "These entries summarize themes and positions expressed by Craig Stueber. Full articles are available on Medium under the same titles."
}
